# Intro to nerual networks

* neurons are basically computational units that take inputs (dendrites) as electrical inputs (called "spikes") that are channeled to outputs (axons).

* inputs/dendrites are x1...xn, x0 is called "bias unit" and always equal to 1
* The same sigmoid(logistic) activation function.
* thetas are called weights (just another name).
* layer 1 is called "input layer"
* intermediate layers are called "hidden layers"
* the last layer must output something, so it's called "output layer"
* theta<sup>j</sup> is the matrix of weights that mapping from layer i to i+1
